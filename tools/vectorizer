#!/usr/bin/env python3

import os
import sys
import argparse
import torch
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from linformer import LinformerEncoder
from tokenizers import Tokenizer, trainers
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import Whitespace


def create_linformer_model():
    """Create and initialize Linformer model for vectorization"""
    model = LinformerEncoder(
        dim=64,
        seq_len=700,
        depth=2,
        heads=4,
        k=64,
        one_kv_head=True,
        share_kv=True,
    )
    return model


def create_word_tokenizer():
    """Create a simple word-level tokenizer"""
    tokenizer = Tokenizer(BPE())
    tokenizer.pre_tokenizer = Whitespace()
    return tokenizer


def create_bpe_tokenizer():
    """Create a BPE tokenizer with trainer"""
    tokenizer = Tokenizer(BPE())
    trainer = trainers.BpeTrainer(
        vocab_size=30000,
        min_frequency=2,
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
    )
    return tokenizer, trainer


def train_bpe_tokenizer(tokenizer, trainer, text):
    """Train BPE tokenizer on input text"""
    tokenizer.train_from_iterator([text], trainer=trainer)
    return tokenizer


def load_bpe_tokenizer(path):
    """Load a saved BPE tokenizer"""
    return Tokenizer.from_file(path)


def process_input(text, tokenizer, model):
    """Process a single line of text into a vector"""
    # Tokenize input
    encoding = tokenizer.encode(text)
    tokens = encoding.ids

    # Pad or truncate to fixed length
    max_length = 700
    if len(tokens) > max_length:
        tokens = tokens[:max_length]
    else:
        tokens = tokens + [0] * (max_length - len(tokens))

    # Convert to tensor and get embedding
    tokens_tensor = torch.tensor(tokens).unsqueeze(0)
    with torch.no_grad():
        vector = model(tokens_tensor)

    # Average pooling over sequence length
    vector = vector.mean(dim=1).squeeze().numpy()
    return vector


def process_log_file(text, tokenizer, model):
    """Process log file line by line"""
    lines = text.strip().split("\n")
    vectors = []

    for line in lines:
        if line.strip():
            vector = process_input(line, tokenizer, model)
            vectors.append(vector)

    return lines, vectors


def main():
    parser = argparse.ArgumentParser(description="Vectorize log data using Linformer")
    parser.add_argument(
        "-bpe",
        type=str,
        help="Path to BPE tokenizer JSON file or to save new BPE tokenizer",
    )
    args = parser.parse_args()

    # Read input text
    text = sys.stdin.read()

    # Initialize model
    model = create_linformer_model()

    # Handle tokenizer initialization
    if args.bpe:
        if args.bpe.endswith(".json") and os.path.exists(args.bpe):
            # Load existing BPE tokenizer
            tokenizer = load_bpe_tokenizer(args.bpe)
        else:
            # Create and train new BPE tokenizer
            tokenizer, trainer = create_bpe_tokenizer()
            tokenizer = train_bpe_tokenizer(tokenizer, trainer, text)
            # Save the trained tokenizer
            tokenizer.save(args.bpe)
    else:
        # Use default word tokenizer
        tokenizer = create_word_tokenizer()

    # Process the input text line by line
    lines, vectors = process_log_file(text, tokenizer, model)

    # Convert vectors to list format for proper serialization
    vectors_list = [v.tolist() for v in vectors]

    # Create DataFrame
    df = pd.DataFrame({"text": lines, "vector": vectors_list})

    # Write to parquet file using pyarrow
    import pyarrow as pa
    import pyarrow.parquet as pq

    # Convert pandas DataFrame to Arrow Table
    table = pa.Table.from_pandas(df)

    # Write to a temporary buffer first
    buffer = pa.BufferOutputStream()
    pq.write_table(table, buffer)

    # Write the buffer contents to stdout
    sys.stdout.buffer.write(buffer.getvalue().to_pybytes())


if __name__ == "__main__":
    main()
